{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d529eb",
   "metadata": {},
   "source": [
    "# Variational Quantum Classifier #\n",
    "\n",
    "## MITP 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43cb93",
   "metadata": {},
   "source": [
    "In this notebook we will consider the Variational Quantum Classifier (VQC) which is a hybrid classical-quantum computing approach to classification problems. The algorithm uses the variational method to optimise a parameterised quantum circuit and minimise a loss function. Comparing to a classical classification problem, the VQC is very similar, but one replaces the classical model with a quantum circuit. \n",
    "\n",
    "Here we will consider a simple VQC, using the Titanic dataset from the [Kaggle competition](https://www.kaggle.com/c/titanic). We will code the algorithm in the `PennyLane` language (See the Introduction to Quantum Computing notebook for help), making use of the inbuilt `AdamOptimiser`. For the classical processing, we will use the `sklearn` package. We will also use `Pandas` `DataFrames`, if you are not familiar, there is a good introduction [here](https://www.w3schools.com/python/pandas/pandas_intro.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9a3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from pennylane import numpy as npl\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "### Set up device\n",
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "dev      = qml.device(\"default.qubit\", wires = n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a963522",
   "metadata": {},
   "source": [
    "# Titanic Classification Problem \n",
    "\n",
    "The Titanic classification problem takes passenger data from the Titanic and predicts which passengers survived the sinking of the Titanic. The data consists of personal data of each passenger, for example their name, age, gender, socio-economic class etc. Try using a `Pandas` `DataFrame` to examine the full dataset in the `train.csv` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269e6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Pandas code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c28d0f",
   "metadata": {},
   "source": [
    "For this VQC example we will choose variables which we know will allow for the algorithm to perform well, namely:\n",
    "\n",
    "- `is_child` (i.e. under 12 years old)\n",
    "- `Pclass_1` (Passenger in first class)\n",
    "- `Pclass_2` (Passenger in second class)\n",
    "- `Sex_female` (Passenger is a woman)\n",
    "\n",
    "We have chosen these variables because we know that children and women from the higher classes had a better survival rate on the Titanic. \n",
    "\n",
    "Having defined our problem, we are now ready to construct the VQC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe79fd",
   "metadata": {},
   "source": [
    "## Constructing the VQC\n",
    "\n",
    "### Data Embedding\n",
    "\n",
    "To construct a VQC one needs to embed a parameterised quantum circuit into the classical classifier toolchain. As quantum computers work in a different paradigm from classical computers, this is more complicated than simply replacing the classical model with a quantum model. To adjust from classical data to quantum data, the quantum part of the algorithm therefore must contain a data encoding stage, often called the 'feature map' stage. Introductions to feature mapping and data encoding are available from Pennylane [here](https://pennylane.ai/qml/glossary/quantum_feature_map) and [here](https://pennylane.ai/qml/glossary/quantum_embedding). Qiskit also has a [quantum machine learning package](https://qiskit.org/ecosystem/machine-learning/) which can be used for similar examples.\n",
    "\n",
    "Since we have only four variables, which are all Booleans, we can use a very simple embedding, *Basis Embedding*. Basis embedding associates each input with a computational basis state of a qubit system. Thus, we can encode the Booleans as $\\vert 0 \\rangle$ for `False` and $\\vert 1 \\rangle$ for `True`. Thus, if a passenger is a female child who is in first class, the embedding would take the form \n",
    "\n",
    "$$\n",
    "1101\\rightarrow \\vert 1101 \\rangle.\n",
    "$$\n",
    "\n",
    "We use the `BasisEmbedding` from Pennylane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f07a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataEmbedding(x):\n",
    "    qml.BasisEmbedding(x, wires=range(0, n_qubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab3425",
   "metadata": {},
   "source": [
    "### Constructing the model\n",
    "\n",
    "Now that we have encoded the data onto the device, we can now build out model (also known as the ansatz). For the VQC, the model is a parameterised quantum circuit with a set of features which can be trained and updated by the optimiser. When constructing a suitable model, one needs to consider two circuit characteristics:\n",
    "\n",
    "- Expressibility: We define expressibility as a circuitâ€™s ability to generate (pure) states that are well representative of the Hilbert space.\n",
    "- Entanglement: How entangled the system is.\n",
    "\n",
    "More information on calculating the expressibility and entanglement of a circuit is available in this [paper](https://arxiv.org/abs/1905.10876).\n",
    "\n",
    "A single Layer of the chosen model has the form (note, here we use qiskit only for its circuit visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe44b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAADWCAYAAAAtmd5RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAopUlEQVR4nO3dfVxUZf7/8dfMgCKiCCIiaMqNUKAQaiqaC5olqBVWWuq6aaSmbmqutWvmlqvi72tUftvcyrL4rqaW5qarUnmDWKK5arqpGQreoXgHiqCgwszvj1MowsARZ+ac0c/z8SDlzJnrvDvCfOZc13WuMVgsFgtCCCGECkatAwghhHAeUjSEEEKoJkVDCCGEalI0hBBCqCZFQwghhGpSNIQQQqgmRUMIIYRqUjSEEEKoJkVDCCGEalI0hBBCqCZFQwghhGpSNIQQQqgmRUMIIYRqUjSEEEKoJkVDCCGEalI0hBBCqCZFQwghhGpSNIQQQqgmRUMIIYRqUjSEEEKoJkVDCCGEalI0hBBCqCZFQwghhGpSNIQQQqgmRUMIIYRqUjSEEEKo5qJ1AL37ZSMUndHm2I18IaxX3Z6rVW5nzAy3l1uIu4kUjVoUnYELuVqnuHXOmNsZMwtxt5HuKSGEEKpJ0RBCCKGadE8JIYTOWCxQVAolV8FoBC93cDFpnUohRUMIIXSg3Az7T8APOXDkHBSXXn/MZIQWnhAeAN3aQhN37XJK0bCRP70fx89Ht2IyuWI0mvDzCmTIQ1OJjRqodTSrJLMQ+pB1Cj7/AfKLq3+83Ay555Wv9fuge1voHw31NXgFl6JhQ0N7T2No79coLy9jZeZ7zF48hJCAaAJ8QrSOZpVkFkI7Zgv8+0dI//nWnvNdFuw/CSPjwM/TbvGqJQPhdmAyuZDQZSTl5jKyT+7WOo4qklkIx7JYYNn2WysYN8ovhr+vg1OFts1VGykadnCt7CqrM98HoKVPqMZp1JHMQjjW1kPKV03mDlW+rLl0BT7ZDFfLbJutJtI9ZUOLN8xiWUYKJVeKMJlcmTTwY4L8IwFI276A9TsXVuybV5BD+8AeTBnymVZxgZozJ382hF7RQ+ga3h+A11MTeTRmLJ3CHtEyslOeZ2ssFjAYtE4hHO38JVi5yzZtnbkIX/8XHutgm/Zqo+srDbPZTEpKCm3btsXNzY2oqCgyMjIICwtj1KhRWserYshDU/lqxgWWv3GOzvf2Zc+h9IrHEjon8daYTbw1ZhNThy7FrV5DRsTP0jCtoqbMYx6fS+o30yi5Usx3P62goZun5gUDnPM83+h0IXyxHf7yBby0GF5fAWn/rTxbRtzZ0n+GKza8Otj8i3LV4Qi6LhpJSUnMmDGD0aNHk5aWxqBBgxg8eDA5OTl07NhR63hWNXL3YtLAj/nhwBoy966s9JjZbGb2kqEkJczGz7uNNgGrUV1mLw9fBjw4gXkrx7N4w0xeeOwdjVNW5ozn+eeT8OZa2HoQSq8p2wpL4JuflO3nirTNJ+zvahlsz7Ftm2Vm+CHbtm1ao9uisWTJElJTU1m1ahWTJ0+mZ8+eTJ06lZiYGMrKyujQwUHXYnXU2N2bJ3tM4pOvX8VsNldsX7huOoF+7eneLlG7cFZUl7nPA8PJPZtFYvfxNHb31jhhVc50notLlf7ncjNYqnn8Yiks2Kx0WYk715Fz198w2NKBPNu3WR3dFo3k5GTi4+OJjY2ttD0kJARXV1ciI5U+7CNHjhAbG0toaCjt27fnu+++0yJutQb0mEDBxTzW7fwnALsObmBn1reM7DdH42TW3ZwZwL9piK6nszrLed6WDdfKqy8YoBSLvAuQc9aRqYSj5RbYp93jBY55w2GwWPT3viY3N5dWrVqxYMECnnvuuUqPDR48mAMHDvDjjz8C0KdPHx5//HHGjh1LZmYmAwcO5PDhw9SrV6/GYxhUjj6mvJBOVHBcnf4/blRw8RSTP+xJclKa6u6SPdmbmPxBzzodz1a5AeYsHU7fLs/TLvDBWvfVOnNdzjPcXm61npyaTkDY7zAYrb9Xs1jM7Pj3/yPzi6l2zSK0E/fse0Q9PK7StppmSNVk4k3zO94f6cnVkot1akttKdDl7KncXGV9bD8/v0rbS0pKyMjIICEhAYBz587x/fffs2rVKgC6deuGv78/6enp9OnTx7Gha7Fo/QwulRby5ufDK7a1ahbGxKc+1C7UHUjP59nkUr/GggGAxYLRVPMbHuHcjEb7LSJlsGPbFcfQ45XGoUOHaNu2Le+88w4TJ06s2D59+nTeeOMN5s2bx9ixY9m1axdPPfUUOTnXR5UGDRpE7969bTa7asdS7T7joUlL6PRM3Z6rVW5nzAy3l1utpduUwcrafuEGd4UuwfbNIrSzZjes26du39+uQG6+oqiO0QBvPqOsU2VPurzSCAoKIjIykuTkZLy9vQkICGD58uWsXbsWQNczp4SwpntbZVyjJvVdILq1Y/IIbQTYaT6JfxP7FwzQ6UC40Whk2bJlREREMGbMGEaMGIGPjw/jxo3DZDJVDILfc889nD59mitXrk9QPnz4MK1by2+d0J9WTaGHlRvXfxthG9gZ6unyrZywlTY+1/+9bdpuMzs0Wg3d/niGhoaSnp5eaduwYcMIDw+nQYMGAPj4+NC9e3cWLFhQMRB+4sQJeva074CmEHU1oBN4usPG/XD56vXtTT2UO3ojW2mXTThGE3e4z19ZcNCWujqoS1O3RaM6O3bsoGvXrpW2ffDBBwwfPpy5c+dSr149lixZUuvMKUf7YtObZO5bSXOv1rz8dCouJletI1VxquAIeQU5RIf0coq81hw+tZf//fIFLBYLE554v2J5Eb0wGqB3BMTdC5OXKtvGPwyBzWQ5kbtJ7L22LRohzaGlg26j0mX3VHWKi4vJysqqclNfUFAQmzdvJisri71791a5r0Nr54vPsDs7nbnjviewRSRb9n6ldaRqnT5/hN2HNjpNXmv+7+tpvDp0CdOGfUHqN9O0jmPVjZ/CFuQrBeNuE9YCOrWxTVsuJhjU2TZtqTqe4w51ezw8PCgvL9c6Rq32ZG9i8YZkTEYT18qukNDleaKC4gDo0LY3G3d9pssPDFqzbT77jmxh8YZZPN/3fwB9552/+mUe7vgHCopOMX/1y3w4aTdzlj7L8bO/4NtE6eMpLr2gbUghajCgExzNh7M1LB2jZtbUk53At7HtctXGaYqGc7GQ/Hwam3Z/zsn8bBr9uvxGQzdP3b6Q9es6ihZNg2jdPJxLpcrNQXrOG966G/uOZFJQlEdTT38ulxZxvug0ng19ru+kv9nkQlRoWB/G9YZ/bFBWqq2LxI4Q4+DFGpyme8qZhPhHAxDsfz9pP3zE5V9fhC+XXsTDrYmGyWrX0M3TKfJGBHZn/9FM8vJzeCh6KJn7VuLVqHmlfh6DQX68hb41cYeX+tz6ILZXQxj7kDI25mjyW2UH2Xl7AMjJ28MjnYbz35wMAHYdXM99rbvW9FTNmIyumM3lhLZ6wCnyenn4kn8xD6PRRESb7izLSCG8dTcaN/Dm7IVczhWexN3NgdfsQtRRg3rwTFdlQkR0a2WyhDXNGkFiB/hzPwj1s76fPUn3lB24mFyZ8lE8V8tKef0PX+K6/WMmznsQ3yb38ESPiVrHq1agXzs+SZuifGhR0O90nxegaeMWBLWIxM+7DYWXzhLRphvhrWOYuehpAF4cME/jhEKoF+SrfF2+qixqeOL89Q9q+n03aOUNzRrXXFQcQZfLiOjJrS5tsSd7E7sOrmdE/MzbPrYzLsnhjJnBMcuI3Oy3Qc66LlYn7nx6/BmR7ikhhBCqSfeUjUUFx9lsSXIhhNAbKRq1aOTrnMfWKrczZtb62EI4EykatQjrpXWCunHG3M6YWYi7jYxpCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE1Wua3FLxuh6Iw2x27kW/eVX7XK7YyZ4fZyC3E3kaJRi6Iz2n0E6e1wxtzOmFmIu410TwkhhFBNioYQQgjVpHtKCAcqvQYH8uB4/vVtqd+BfxO4xwfaNgeTvJUTOiZFw0b+9H4cPx/disnkitFows8rkCEPTSU2aqDW0aySzI5z4TJ8uxd2HIarZZUf231M+QLwbADd2kLP+6Ce/HYKHZIfSxsa2nsaQ3u/Rnl5GSsz32P24iGEBEQT4BOidTSrJLP9bc+BFTuUq4zaFJZA2n+V4jIkBgKb2T+fELdCLoTtwGRyIaHLSMrNZWSf3K11HFUks32s3QOLt6orGDc6WwTvrYefjtsnlxB1JUXDDq6VXWV15vsAtPQJ1TiNOpLZ9jb/onRJ1VW5GVK/hxyN7l0RojrSPWVDizfMYllGCiVXijCZXJk08GOC/CMBSNu+gPU7F1bsm1eQQ/vAHkwZ8plWcYGaMyd/NoRe0UPoGt4fgNdTE3k0Ziydwh7RMrJTnOfThbBqV837zB2q/DmxhmjlZli8DV7pK2McQh90faVhNptJSUmhbdu2uLm5ERUVRUZGBmFhYYwaNUrreFUMeWgqX824wPI3ztH53r7sOZRe8VhC5yTeGrOJt8ZsYurQpbjVa8iI+FkaplXUlHnM43NJ/WYaJVeK+e6nFTR089S8YIBznOcVO6HMbJu2zhXBxp9t09btuFaujLX8+0el2y3nDFgsWqcSjqbropGUlMSMGTMYPXo0aWlpDBo0iMGDB5OTk0PHjh21jmdVI3cvJg38mB8OrCFz78pKj5nNZmYvGUpSwmz8vNtoE7Aa1WX28vBlwIMTmLdyPIs3zOSFx97ROGVlej3Ppy/CL3m2bTPzoHLVoZXdR+GvX8KiTNiwX+l2e3cdpKQpRU3cPXRbNJYsWUJqaiqrVq1i8uTJ9OzZk6lTpxITE0NZWRkdOnTQOmKNGrt782SPSXzy9auYzdd/2xeum06gX3u6t0vULpwV1WXu88Bwcs9mkdh9PI3dvTVOWJUez/POw7Zv82IJZJ2yfbtq7MtVxlZKqhnMP3kB/r4OikodHktoRLdFIzk5mfj4eGJjYyttDwkJwdXVlchIpQ/7r3/9K6GhoRiNRpYvX65FVKsG9JhAwcU81u38JwC7Dm5gZ9a3jOw3R+Nk1t2cGcC/aYhup7OC/s7z0fza99FTuzWxWGDlj2Co4fHCEvg+y6GxhIZ0ObSWm5vL3r17eemll6o8duzYMSIiIqhfvz4A8fHxDB8+nOeee87RMSt5a8ymKtsaujVmxd8KACi4eIr3vvojyUlpuLrUc3C66tWWWY+c4TznXbBTu+ft025NjubDmYu175d5EBIi7Z9HaE+3RQPAz8+v0vaSkhIyMjJISEio2NatW7c6HcNgsPbeqbKUF9KJCo6r0zFutGj9DC6VFvLm58MrtrVqFsbEpz60+pyMjE08MLhnnY5nq9y3SuvMdTnPcHu5b/bCR4XUb9C44vvfZklZY+3xm2dVrVzzNc/FJlS/s52EPPAk/SbUfgVfVAoGo1FGxm1swiLlfKp9vbodFpX/drosGj4+PgBkZWXRt2/fiu1z5swhLy9P14Pg1ox/Yh7jn5indYw6eeWZVK0jqKaH82wuu+pU7dbkakmhqv2uXbkkBeMuocuiERQURGRkJMnJyXh7exMQEMDy5ctZu3YtgE2KhtqqumOpdp/xEBsbh+X9uv0iapXbGTPD7eW+2dxv4Mi5699buw9DzX0aN0oa+hir3nLsC3NZObz+L7h0xfo+BqBHeEPmSdGwud9+NtS+XjmCLgfCjUYjy5YtIyIigjFjxjBixAh8fHwYN24cJpOpYhBcCD1qZadJZvZqtyYuJmXxRGsMgNEIsfc6LJLQmC6vNABCQ0NJT0+vtG3YsGGEh4fToEEDjVIJUbvIe+A7G88mcjXBvf62bVOtXuHKKr3fZylF4sb3vCYjjOgBLZpok004ni6vNKzZsWNHla6padOm0bJlS7Zu3cro0aNp2bIl2dnZGiWsqqz8GuP/HsOjUz04ce6Q1nGsOlVwhB8PbXSavNZ8tmEWT8/w59OvX9MsQ4gvNG9c+363omMbcNdo0p3RAE89AJPi4YGg69v7RsJfEyGipTa5hDacpmgUFxeTlZVV5aa+GTNmkJuby5UrV8jPzyc3N5fg4GCNUlZlMrowffhX9Gj/lNZRanT6/BF2H9roNHmt6dv5eaYM1nY9L4MBHrfhvaf1XaBPe9u1V1f3NFWWa//NI+2hsVz033Wcpmh4eHhQXl7Oiy++qHWUGu3J3sSf5z/Cqx8n8PIHvSgqOY9Xo+Zax6rVmm3zWb9zIa/M7+0UeeevfpnDeT+xM2sdo9++H4A5S58Fg8Eh0xNrEx4AnYNq30+NxI7g1dA2bQlxu3Q7puHcLCQ/n8am3Z+zdtt8nun1F60D1apf11G0aBrEiPiZWkdRJbx1N/YdyaSgKI+mnv5cLi3ifNFpvDx8OXZ6v9bxABjYWRkLsLb8h5pZU73Coat+LpyFcJ4rDWcS4h8NQLD//ZzId75xAWcQEdid/UczycvP4aHooWTuW6m7KyRXE4yMq9sVh8modHE9er/S3SWEXkjRsIPsvD0A5OTtwb+pc7xNNBldMZvLtY6hmpeHL/kX8zAaTUS06c6yjBTCW9dtdQB7cjUp4wAj48BX5eB42+YwOUGZ6ioFQ+iNdE/ZgYvJlSkfxXO1rJTX//AlMxYOYu+R7zlx7iBPx71Ct3aPax2xikC/dnySNoWZi57GYrHoPi9A08YtCGoRiZ93GwovnSWiTTfSti/g35n/oOhyAUWXz2t+d/hvIgIg3B8OnYY9x+B4AZwpUm6eq+8C/l7KfRgPBIGfp9ZphbBOioYdBPvfX2lsYNqwLzRMo07DBp68PXaz1jFuySvP/F/F35dOOwFAYIv2JHRO0ipSjQwGaOunfAnhrKR7SgghhGpypWFjUcFxmqwuK4QQjiBFoxaNfJ3z2FrldsbMWh9bCGciRaMWYb20TlA3zpjbGTMLcbeRMQ0hhBCqSdEQQgihmhQNIYQQqknREEIIoZoUDSGEEKpJ0RBCCKGaFA0hhBCqSdEQQgihmhQNIYQQqknREEIIoZoUDSGEEKpJ0RBCCKGaFA0hhBCqySq3tfhlIxSd0ebYjXzrvvKrVrmdMTPcXm4h7iZSNGpRdAYu5Gqd4tY5Y25nzCzE3Ua6p4QQQqgmRUMIIYRqUjSEEKqUm+Hk+evf5xeDxaJdHqENGdMQQlh1rRx+PArbDsGxfCgzX39sxkpoWB/C/ODBUAhsBgaDdlmFY0jRsJE/vR/Hz0e3YjK5YjSa8PMKZMhDU4mNGqh1NKsks6jJ/hPw+Q9QWGJ9n0tXYNdR5SvUD57pAt4ejssoHE+6p2xoaO9p/HtWMSveyKfPA8OZvXgIJ84d0jpWjSSzuJnZAit2wPxNNReMm2Wdgv9ZA/tkBtwdTYqGHZhMLiR0GUm5uYzsk7u1jqOKZBagjFEs3w6bf6nb86+UwYLNsO+EbXMJ/ZCiYQfXyq6yOvN9AFr6hGqcRh3JLAD+cxgya7lomztU+bLGbIGFW+DCZdtmE/qg6zENs9nM22+/zYcffsjx48cJCwvj3XffZdSoUcTGxjJ//nytI1ayeMMslmWkUHKlCJPJlUkDPybIPxKAtO0LWL9zYcW+eQU5tA/swZQhn2kVF6g5c/JnQ+gVPYSu4f0BeD01kUdjxtIp7BEtIzvleXYGF0vgXzts01bpNfjiBxjV0zbt3a7Sa3CtDNzrg0neKt8WXReNpKQkVqxYwbRp0+jYsSOZmZkMHjyYs2fPMmnSJK3jVTHkoakM7f0aRZfP89ayJPYcSiehcxIACZ2TKv5ecPEUkz/syYj4WVrGBWrOPObxuUz5qA9RwXHsyPqWhm6emhcMcM7z7Ay2HISSa7Zrb/9JOHEeArxs1+at+vkkbNwPB08r37u5Qpdg6BUOng20y+XMdFtzlyxZQmpqKqtWrWLy5Mn07NmTqVOnEhMTQ1lZGR06dNA6olWN3L2YNPBjfjiwhsy9Kys9Zjabmb1kKEkJs/HzbqNNwGpUl9nLw5cBD05g3srxLN4wkxcee0fjlJU543nWq3IzbD1o+3a32KFNtTIOwIfpcOiG9cxKrynb305T7jMRt063RSM5OZn4+HhiY2MrbQ8JCcHV1ZXIyEjOnz9P//79CQ0NJSoqikceeYRDh/Qxi6axuzdP9pjEJ1+/itl8fXL7wnXTCfRrT/d2idqFs6K6zH0eGE7u2SwSu4+nsbu3xgmrcsbzrEenC+Fiqe3bPXjK9m2qkVsA/9qp/L26GxAvlsCiTMdmulPosmjk5uayd+9eBg6sOvf+2LFjREREUL9+fQwGAxMnTiQrK4s9e/bQv39/RowYoUHi6g3oMYGCi3ms2/lPAHYd3MDOrG8Z2W+OxsmsuzkzgH/TEAJ8QjRMVTNnPM96c7zAPu2eLYKSq/ZpuybfZ0FN9xlagMNnle4zcWsMFov+FgLYtm0bMTExrFmzhr59+1ZsLykpITg4mISEBBYsWFDleTt27CAxMZHc3NonihtU3rqa8kI6UcFxqrNb81v/enJSmurukj3Zm5j8Qd1GEm2VG2DO0uH07fI87QIfrHVfrTPX5TzD7eW+E3ROfI2Yp2ZU2lbTDKmaTLxpzsE/X7mP8ycP1DFZ3Qx/JwfPZoG17pexcAK7v3nXAYnqZsIi5eX5f39v/1vt1ZYCXQ6E+/j4AJCVlVWpaMyZM4e8vDw6duxY7fPmzp1LYmKiIyLeskXrZ3CptJA3Px9esa1VszAmPvWhdqHuQHKe68p+L0oGO7ZtjdFoUrWfQeV+4jpdXmmYzWaio6PJy8sjJSWFgIAAli9fztq1azl27Bjbtm2jS5culZ4zffp00tLS2LhxI+7u7jbLsmOpdp/x0KQldHqmbs/VKrczZobby30n2JIFy/6jbt/frkBuvqKwZuaT4OFWt1x19elm+G9u7Qsq/rE3hDR3TKa6+O0c1/Wqzx50OaZhNBpZtmwZERERjBkzhhEjRuDj48O4ceMwmUxERkZW2n/mzJmsXr2ar7/+2qYFQ4i7RUs7zXFo4u74ggHQPbTmgmEwgG9jCPZ1XKY7hS67pwBCQ0NJT0+vtG3YsGGEh4fToMH1CdbTp09n7dq1rFu3jiZNmjg4pRB3Bn8v5R6GUhvepwHavSi3ba6svPt9VtXHDAZwMcLvu8mqvHWh26JRnR07dtC1a9eK7/ft28cbb7xBcHAwcXFxFdt3797t+HBCODFXEzwQBN/Vcc0pa2I0mnRnMMCTnaBZI0j/ufKSJve2gP73a3vToTNzmqJRXFxMVlYWY8eOrdgWERGhesRfK3kFh5mz9A8YMODj2ZI/D16ISYeDb6cKjpBXkIOfd6BT5K3O3OWjOXxqLwaDgfED/lGxtIhQ53ehkHlQudHPFlp5a9v9YzBA7L3QIxQmLVG2vZ4IXg21y3Qn0OWYRnU8PDwoLy/nxRdf1DrKLfFwa8LMEat5e+xm/LwD2X5grdaRqnX6/BF2H9roNHmr83Svv/C/f9zC5EGfsnDddK3jOJ1mjSG+vW3aMhlhcFd9dP8Yb3iVk4Jx+5ymaDiLPdmb+PP8R3j14wRe/qAXFiw0bOAJgIvJFaNBn+/a12ybz/qdC/nbwqd0n3f+6pc5nPcTO7PWMfrt+wGYs/RZ3Ooprwguv35Ak7h1vcKV7puaTPys9plTAzoq4yTiziNFwy4sJD+fRr+uo1m7TVmJ91zhSXZmraNTqPYL/lWnX9dR9O44jDdHbwD0nTe8dTf2Hclk35EtNPX053JpEeeLTuPlofSFLEibwoAHx2uc0jmZjPDc7yDcv27PN6AUjAdlpfo7lhQNOwjxjwYg2P9+TuQf4mrZFd78/FkmDfwIk0n/w0h6zxsR2J39RzPJy8/hoeihZO5biVcjZbL9iu/m0to3XNXd66J69Vzg+VhI7AAut3DB5tMIXnxYGUcQdy79vSLcAbLz9gCQk7cH/6bBzF0+ise6jaN183CNk1lnMrpiNpcD6D6vl4cv+Rfz8PEMIKJN918/52MMO375ln1HMnnt959rHdHpGY0Qdx9E3aMMjm/NhmIrCxq29IYH20KHNkrBEXc2+Se2AxeTK1M+iudqWSkj4mexZGMyp88fZcV3cxnw4AQebD9A64hVBPq145O0Kfzloz7sP5qp+7xNG7cgqEUkft5tKLx0log23fjbwqdwr9+YyR/0lKVDbMSrIfS7HxKiIL8Ics/D5SvKALdXQ2WGlBY37wntSNGwg2D/+xkRP7Pi+1UzizRMo07DBp68PXaz1jFUe+WZ/6v4+9JpygdSf/qKjW8yEBWMBmV2VbPGWicRWpMxDSGEEKrJlYaNRQXH2WxJciGE0BspGrVopOEdrbdzbK1yO2NmrY8thDORolGLsF5aJ6gbZ8ztjJmFuNvImIYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVZJXbWvyyEYrOaHPsRr51X/lVq9zOmBluL7cQdxMpGrUoOgMXcrVOceucMbczZhbibiPdU0IIIVSToiGEEEI16Z4SQtyRzlyEfSfgeP71be+tB/8m0MYH2rWEevIKeMvklNnIn96P4+ejWzGZXDEaTfh5BTLkoanERg3UOppVklnciY6cg7Q98Mupqo8dOq18bf4FGtSDmBB4pB24uTo+p7OSomFDQ3tPY2jv1ygvL2Nl5nvMXjyEkIBoAnxCtI5mlWQWd4pyM6zZA+n7waJi/5KrsHE//HgUhsZASHO7R7wjyJiGHZhMLiR0GUm5uYzsk7u1jqOKZBbOrNwMn36nFAE1BeNG5y/B+xvgp+N2iXbHkaJhB9fKrrI6830AWvqEapxGHcksnNny/8De25iuXW6B1O/hWH7t+97tpHvKhhZvmMWyjBRKrhRhMrkyaeDHBPlHApC2fQHrdy6s2DevIIf2gT2YMuQzreICNWdO/mwIvaKH0DW8PwCvpybyaMxYOoU9omVkpzzPwn72n4Cth2reZ+5Q5c+JNfwYlJth8Vb4UwK4mmyX706j6ysNs9lMSkoKbdu2xc3NjaioKDIyMggLC2PUqFFax6tiyENT+WrGBZa/cY7O9/Zlz6H0iscSOifx1phNvDVmE1OHLsWtXkNGxM/SMK2ipsxjHp9L6jfTKLlSzHc/raChm6fmBQOc8zwL+zBbYMUO27V3qhC2ZNmuvbq6cBm++en69/tPgNmsXZ4b6bpoJCUlMWPGDEaPHk1aWhqDBg1i8ODB5OTk0LFjR63jWdXI3YtJAz/mhwNryNy7stJjZrOZ2UuGkpQwGz/vNtoErEZ1mb08fBnw4ATmrRzP4g0zeeGxdzROWZkznmdhW7/kwbli27a55aBSjLRgsUDaf2H6V8qfv5m/CWavhnNF2uS6kW6LxpIlS0hNTWXVqlVMnjyZnj17MnXqVGJiYigrK6NDhw5aR6xRY3dvnuwxiU++fhXzDW8RFq6bTqBfe7q3S9QunBXVZe7zwHByz2aR2H08jd29NU5YlTOeZ2E7O4/Yvs2zRZXv7XCk9J+VKwxLNUXrXLFyn8nlK47PdSPdFo3k5GTi4+OJjY2ttD0kJARXV1ciI5U+7MTERCIjI4mOjqZz586sX79ei7jVGtBjAgUX81i3858A7Dq4gZ1Z3zKy3xyNk1l3c2YA/6Yhup7O6oznWdiGvQaujxfYp92aXC2Db/Zaf9xiUbqtfshxXKbq6HIgPDc3l7179/LSSy9VeezYsWNERERQv359AFJTU2nSpAkAP/74I3FxcRQUFGAyOXYk660xm6psa+jWmBV/U376Ci6e4r2v/khyUhquLvUcms2a2jLrkTOeZ2Ef5WY4e9E+beddsE+7Ndl3Aq5cq32/H7Kh5332z2ONbosGgJ+fX6XtJSUlZGRkkJCQULHtt4IBUFhYiMFgwFLdtd1NDAaDqiwpL6QTFRynat+aLFo/g0ulhbz5+fCKba2ahTHxqQ+tPicjYxMPDO5Zp+PZKvet0jpzXc4z3F5uoQ2X+u6MW3Cp0rbfZklZY+3xm2dVfbTgUwZ1ee420t26qIf/SNyzf691v8O5ZzEYfG1+fDWvm6DTouHj4wNAVlYWffv2rdg+Z84c8vLyqgyCjxs3jrS0NAoLC/nyyy9xcdHf/9b4J+Yx/ol5Wseok1eeSdU6gmrOfJ7FrTGXXQWUFzu1bwLVKrtaatP21CgpPlfrPhaLmZKi2vezJ4NFbXlxILPZTHR0NHl5eaSkpBAQEMDy5ctZu3Ytx44dY9u2bXTp0qXK8zIyMnjppZfYvHkzHh4eNsmyY6l2n/HQpCV0eqZuz9UqtzNmhtvLLbQza5UycF0bNfdp3OiJTvC7sLrnqovSa/DXFcrYRk36RcHD7RyTqTq6HAg3Go0sW7aMiIgIxowZw4gRI/Dx8WHcuHGYTKaKQfCbxcbGYjQa2bJli4MTCyG00MpOE/rs1W5N3FyhVw1jFQYDeLgpiyxqSX/9OL8KDQ0lPT290rZhw4YRHh5OgwYNACguLiY/P5/WrVsDykB4dnY2992n4SiREMJholvDrqO2bdOrIbRuats21XqkPVy+qqzCC2D49T8WC3g2gNE9lcKhJd0Wjers2LGDrl27Vnx/6dIlnn76aYqLi3FxccHNzY1FixZxzz33aJiyssJL5/jrp49hMrnS0M2T137/OfVdG2gdq4pTBUfIK8ghqEWkU+S1Zt7KCWSf3M21a6WMfvRt2gV21zqSsKPwAPByh/OXbddm97Zg1KgPxmhQusa6t1WWRjlXrCxp0q4lRLUCFx0sb+I0RaO4uJisrCzGjh1bsa158+Zs27ZNw1S182jgxTtjv8doNLLw2+ls279al5/9cPr8EXYf2khkUKxT5LVmdP8UXEyunD5/lHdXjGVW0hqtIwk7MhkhsaOywq0t+HhADwePZVSnuafy/6VHTlM0PDw8KC8v1zpGrfZkb2LxhmRMRhPXyq4w7Q/LK+6kLreUE+DTVuOE1VuzbT77jmxh/9GtvDl6A6DvvPNXv8zDHf9AQdEp5q9+mQ8n7WbO0mcZ2f9NvDx8KblSTJB/lNYxhQNE3QMd29R8d7iaAXCjAQbHQH2neVXUhpweu7CQ/Hwam3Z/ztpt87k/pBfv/mss9VzcGPi7P2kdrlr9uo6iRdMgRsTP5MCx7brPG966G/uOZFJQlEdTT38ulxZxvug0Xh6+vJE6gAPHt/PnwQtrb0jcEZ7pCkWlkFXNp/WpYQCGxECw7W9/uOPocvaUswvxjwYg2P9+TuQf4t57OvOPCTvo3m4AX//nE43T1c4Z8kYEdmf/0Uzy8nN4KHoomftW4tVI+ei1N4b/i7+/+AOfpL2qcUrhKK4mGBkH3eows6iRm/LcToG2TnVnkqJhB9l5ewDIyduDr2eriu0N3RpTT6eDyiajK2ZzOdd+vWEK9J3Xy8OX/It5GI0mItp0Z1lGCuGtu3G1TFnNrUF9D9zqNdQ4pXAkVxMM6gJjeqmb/eRqUorMX/orA+pCHemesgMXkytTPornalkpz/f7Hya9H4vRYKRRA2/ddpkE+rXjk7QpTPm4D2aLWfd5AZo2bkFQi0j8vNtQeOksEW26MWvR0xSXXMBsKScpYbbWEYUGwlooX8fzYe8J5c9zxcpaVQ1cwd8L2vjA/feAe32t0zofXd4Rrie3epfynuxN7Dq4nhHxM2/72M54d7UzZga5I1wItaR7SgghhGrSPWVjUcFxmqwuK4QQjiBFoxaNNJyCdzvH1iq3M2bW+thCOBMZ0xBCCKGajGkIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjVpGgIIYRQTYqGEEII1aRoCCGEUE2KhhBCCNWkaAghhFBNioYQQgjV/j/RmyI6aDkuTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 507.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.visualization import *\n",
    "\n",
    "qc = QuantumCircuit(4)\n",
    "for i in range(4):\n",
    "    qc.rz(Parameter(\"p%i\" %i), i)\n",
    "    qc.ry(Parameter(\"t%i\" %(i)), i)\n",
    "    qc.rz(Parameter(\"w%i\" %(i)), i)\n",
    "\n",
    "for i in range(3):\n",
    "    qc.cx(i, i+1)\n",
    "qc.cx(3, 0)\n",
    "    \n",
    "qc.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04240dc",
   "metadata": {},
   "source": [
    "In Pennylane we build the layer using the `qml.Rot` which has the form \n",
    "\n",
    "$$\n",
    "R(\\phi, \\theta, \\omega) = RZ(\\omega) RY(\\theta) RZ(\\phi) = \\begin{pmatrix} e^{-i(\\phi+\\omega)/2} \\cos(\\theta/2) & -e^{i(\\phi-\\omega)/2} \\sin(\\theta/2) \\\\ e^{-i(\\phi-\\omega)/2} \\sin(\\theta/2) & e^{i(\\phi+\\omega)/2} \\cos(\\theta/2) \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "and thus the Layer can be built easily using Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e9bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        qml.Rot(W[i,0], W[i,1], W[i,2], wires=i)\n",
    "        \n",
    "    for i in range(n_qubits-1):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    qml.CNOT(wires=[n_qubits-1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b418c47",
   "metadata": {},
   "source": [
    "The expressibility of the circuit is introduced through the initial rotations in the Layer, which are parameterised RZ, RY and RZ rotations, the angles of which will be adjusted by the classical optimiser. These rotations construct a superposition of the compuational basis on each of the circuits individually.\n",
    "\n",
    "The circuit is then entangled with the series of `CNOT` gate operations. \n",
    "\n",
    "The Layer can then be applied iteratively, increasing the number of features in the model. The subsequent applications of the Layer increases the expressibility and entanglement of the circuit. However, there are several things that must be considered when deciding how many iterations to apply. Firstly, from a more conventional stance, one does not want to over train by having too many parameters. Therefore, finding a good amount of layers wihtout overtraining is important. Secondly, from a quantum point of view, more iterations result in deeper circuits. In the Noisy Intermediate-Scale Quantum (NISQ) era, circuit depth is one of the most important factors contributing to the fidelity of the results from the device. The deeper the circuit, the more susceptable the circuit becomes to noise. See this [paper](https://arxiv.org/abs/1905.10876) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e97d1",
   "metadata": {},
   "source": [
    "### Building the circuit\n",
    "\n",
    "We are now ready to build the full circuit, including the data embedding stage and the iterative application of the Layers. \n",
    "\n",
    "Here we \"borrow\" the Machine Learning term weights for the parameters of the rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7125698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuitBuild(weights, x):\n",
    "    \n",
    "    dataEmbedding(x)\n",
    "    \n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb72c17",
   "metadata": {},
   "source": [
    "Let us now assume we want to add a classical bias parameter. We add this to the VQC by defining a final model with a classical node that uses the first variable and then feeds the rest into the quantum node. Before this, we reshape the list of remaining variables for easy use in the quantum node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f25006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuitBuild(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed31e0",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "In supervised learning, the cost function is often the sum of the loss function and a regulariser. We will do the same here and use standard square loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c72026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareLoss(labels, predictions):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l-p)**2\n",
    "        \n",
    "    loss = loss/len(labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ca8ff",
   "metadata": {},
   "source": [
    "To keep track of how well the model is performing, we also define the accuracy given the target labels and model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee71ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l-p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    \n",
    "    loss = loss/len(labels)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba248f7",
   "metadata": {},
   "source": [
    "The cost will depend on the data, which here are the features and labels considered in the current iteration of the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b9af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return squareLoss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840368f",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "We are now ready to perform the training of the model for our Titanic classification problem. Here we prepare and process the data from the Titanic `train.csv` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f97d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in file\n",
    "train = pd.read_csv('TitanicData/train.csv')\n",
    "\n",
    "# change data type to a string\n",
    "train['Pclass'] = train['Pclass'].astype(str)\n",
    "\n",
    "# Add columns with Boolean type values for chosen variables\n",
    "train = pd.concat([train, pd.get_dummies(train[['Pclass', 'Sex', 'Embarked']])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4a7fc",
   "metadata": {},
   "source": [
    "The dataset is not complete, and some ages are missing. So we fix the missing ages with the median age, and a value between 0 and 11 for children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91431c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age'] = train['Age'].fillna(train['Age'].median())\n",
    "train['is_child'] = train['Age'].map(lambda x: 1 if x < 12 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b05a2",
   "metadata": {},
   "source": [
    "We now use the `train_test_split` from `sklearn` to split our dataset randomly into test and train subsets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168fbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[model_vars], train['Survived'], \n",
    "                                                    test_size=0.10, random_state=42, \n",
    "                                                    stratify=train['Survived'])\n",
    "\n",
    "X_train = npl.array(X_train.values, requires_grad=False)\n",
    "Y_train = npl.array(y_train.values * 2 - np.ones(len(y_train)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c6e68",
   "metadata": {},
   "source": [
    "Here `requires_grad` marks an argument as trainable. More info on the PennyLane NumPy interface is available [here](https://docs.pennylane.ai/en/stable/introduction/interfaces/numpy.html). \n",
    "\n",
    "Now we have prepared the data, we can optimise the rotation parameters of the model using the `AdamOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ef5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set initial parameters\n",
    "npl.random.seed(0)\n",
    "weights_init = 0.01 * npl.random.randn(n_layers, n_qubits, 3, requires_grad=True)\n",
    "bias_init    = npl.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "229b01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt        = AdamOptimizer(0.125)                 # Set appropriate learning rate for Adam\n",
    "n_it       = 35                                   # Number of iterations\n",
    "batch_size = math.floor(len(X_train)/n_it)        # Determine batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c41639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 2.3009047 | Accuracy: 0.3657928 \n",
      "Iter:     2 | Cost: 2.0069723 | Accuracy: 0.3657928 \n",
      "Iter:     3 | Cost: 1.6768622 | Accuracy: 0.3657928 \n",
      "Iter:     4 | Cost: 1.4171261 | Accuracy: 0.5255930 \n",
      "Iter:     5 | Cost: 1.2510112 | Accuracy: 0.6167291 \n",
      "Iter:     6 | Cost: 1.1688063 | Accuracy: 0.6167291 \n",
      "Iter:     7 | Cost: 1.1336431 | Accuracy: 0.6167291 \n",
      "Iter:     8 | Cost: 1.0991138 | Accuracy: 0.6167291 \n",
      "Iter:     9 | Cost: 1.0469797 | Accuracy: 0.6167291 \n",
      "Iter:    10 | Cost: 0.9871259 | Accuracy: 0.6167291 \n",
      "Iter:    11 | Cost: 0.9374590 | Accuracy: 0.6167291 \n",
      "Iter:    12 | Cost: 0.8986214 | Accuracy: 0.6167291 \n",
      "Iter:    13 | Cost: 0.8667102 | Accuracy: 0.6779026 \n",
      "Iter:    14 | Cost: 0.8460376 | Accuracy: 0.7802747 \n",
      "Iter:    15 | Cost: 0.8202597 | Accuracy: 0.7802747 \n",
      "Iter:    16 | Cost: 0.7953863 | Accuracy: 0.7802747 \n",
      "Iter:    17 | Cost: 0.7581353 | Accuracy: 0.7802747 \n",
      "Iter:    18 | Cost: 0.7289362 | Accuracy: 0.7802747 \n",
      "Iter:    19 | Cost: 0.7101794 | Accuracy: 0.7802747 \n",
      "Iter:    20 | Cost: 0.7028200 | Accuracy: 0.7915106 \n",
      "Iter:    21 | Cost: 0.7021316 | Accuracy: 0.7840200 \n",
      "Iter:    22 | Cost: 0.6965682 | Accuracy: 0.7840200 \n",
      "Iter:    23 | Cost: 0.6967811 | Accuracy: 0.7840200 \n",
      "Iter:    24 | Cost: 0.6911936 | Accuracy: 0.7840200 \n",
      "Iter:    25 | Cost: 0.6814144 | Accuracy: 0.7840200 \n",
      "Iter:    26 | Cost: 0.6754717 | Accuracy: 0.7840200 \n",
      "Iter:    27 | Cost: 0.6717139 | Accuracy: 0.7840200 \n",
      "Iter:    28 | Cost: 0.6683102 | Accuracy: 0.7840200 \n",
      "Iter:    29 | Cost: 0.6668538 | Accuracy: 0.7840200 \n",
      "Iter:    30 | Cost: 0.6682391 | Accuracy: 0.7840200 \n",
      "Iter:    31 | Cost: 0.6663942 | Accuracy: 0.7840200 \n",
      "Iter:    32 | Cost: 0.6654684 | Accuracy: 0.7840200 \n",
      "Iter:    33 | Cost: 0.6629519 | Accuracy: 0.7840200 \n",
      "Iter:    34 | Cost: 0.6627042 | Accuracy: 0.7840200 \n",
      "Iter:    35 | Cost: 0.6651312 | Accuracy: 0.7840200 \n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias    = bias_init\n",
    "\n",
    "### Run Optimisation\n",
    "\n",
    "for it in range(n_it):\n",
    "\n",
    "    # Update trainable arguments with one step of the optimizer.                                                                                                                                   \n",
    "    batch_index         = npl.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch             = X_train[batch_index]\n",
    "    Y_batch             = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # Compute accuracy                                                                                                                                                            \n",
    "    predictions         = [npl.sign(variational_classifier(weights, bias, x)) for x in X_train]\n",
    "    acc                 = accuracy(Y_train, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X_train, Y_train), acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b265bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7712374581939799"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = npl.array(X_test.values, requires_grad=False)\n",
    "Y_test = npl.array(y_test.values * 2 - npl.ones(len(y_test)), requires_grad=False)\n",
    "\n",
    "predictions = [npl.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "### Test model\n",
    "accuracy_score(Y_test, predictions)\n",
    "precision_score(Y_test, predictions)\n",
    "recall_score(Y_test, predictions)\n",
    "f1_score(Y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcf74c",
   "metadata": {},
   "source": [
    "## Exercise: Modify your VQC\n",
    "\n",
    "We see that the model performs well, reaching an accuracy close to 80%. It is interesting to investigate the affect of different parts of the VQC. Below, try using a different model quantum circuit (see this [paper](https://arxiv.org/abs/1905.10876) for inspiration!) with different expressibility and entanglement. How does the model effect the training and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c35168",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def new_circuitBuild(weights, x):\n",
    "    \n",
    "    ###\n",
    "    ### Your code here\n",
    "    ###\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ed262",
   "metadata": {},
   "source": [
    "Currently, the measurement returns the expectation value of the `PauliZ` observable on wire 0. Do you kno why we do this? How does changing the measurement affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3dbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def new_circuitBuild(weights, x):\n",
    "    \n",
    "    ###\n",
    "    ### Your code here\n",
    "    ###\n",
    "        \n",
    "    return ### How does the measurement effect the system?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyQML",
   "language": "python",
   "name": "pyqml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
